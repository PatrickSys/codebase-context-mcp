# Research: Academic Papers on Context Engineering for AI Coding

**Date:** December 2025  
**Purpose:** Synthesize recent arXiv research on LLM context engineering, grounding, and codebase understanding

---

## Executive Summary

Four recent academic papers provide empirical backing for design decisions we've already made. They also reveal approaches we should avoid (at least for MVP).

**Key Validations:**

| Our Design Decision | Research That Validates It |
|---------------------|---------------------------|
| Pattern frequency with exact percentages | "Grounding First, Then Generate" (arXiv:2510.10290) |
| Structured metadata indexing (not raw code) | DeepCodeSeek enriched indexing (arXiv:2509.25716) |
| Golden Files (centrality-based selection) | CACE multi-factor prioritization (arXiv:2506.18796) |
| Internal library/wrapper detection | Code Digital Twin tacit knowledge (arXiv:2503.07967) |

**Key Warnings:**

- Hypothetical query expansion (DeepCodeSeek) requires LLM call per search—too slow for MVP
- Full concept graphs (Digital Twin) require NLP over docs/PRs—Phase 3+ at earliest
- Autonomous agents have ~30-35% success rate—stay human-led

---

## Paper 1: Grounded AI for Code Review

**Source:** arXiv:2510.10290v1, October 2025  
**Key Concept:** "Grounding First, Then Generate" (GFTG)

### The Problem

LLMs generate plausible-sounding code explanations but hallucinate APIs and patterns. Static analyzers produce thousands of warnings with no explanation of *why* they matter.

### The Insight

Combine deterministic static analysis for *recall* (finding the spot) with LLM reasoning for *precision* (explaining the why):

```
Finding ID → AST Slice → Rule ID → LLM Explanation
```

> **"Every AI-generated comment must be explicitly anchored to deterministic signals—compiler-verified builds, static-analysis findings, formal rule definitions, and precise file/line locations."**

### Relevance to Our MCP

This is exactly what we do. Pattern detection is deterministic (AST-based). "97% inject()" is grounded evidence, not an opinion.

| Grounded AI Pattern | Our Implementation |
|---------------------|-------------------|
| Deterministic signals first | AST-based pattern detection |
| Precise file/line citations | Golden Files with exact locations |
| Rule-based evidence | Framework-specific patterns (Angular v17+) |

---

## Paper 2: DeepCodeSeek

**Source:** arXiv:2509.25716v1, ServiceNow Research, ECAI-2025  
**Key Concept:** Intent-aware retrieval + enriched indexing

### The Problem

Standard RAG fails for code due to **vocabulary mismatch**. Developers ask "how to filter users" (intent), but code contains `class UserPredicate` (implementation). Embeddings don't bridge this gap.

### The Insight: Hypothetical Query Expansion

Instead of searching for the developer's query directly:

```
Query: "filter active users"
→ LLM generates: users.filter(u => u.isActive)
→ Search for generated snippet (matches naming patterns)
```

Results: **87.86% top-40 retrieval accuracy**, doubling baseline BM25.

### The Insight: Enriched Indexing

Raw code contains noise. Indexing **JSDoc/signatures** outperforms raw function bodies:

| Index Type | Top-5 Accuracy |
|------------|----------------|
| Raw Code | 36.71% |
| Namespace Grouping + JSDoc | **58.21%** |

### Relevance to Our MCP

**What we already do:** Index structured metadata (component types, pattern classifications, layer assignments), not just raw code chunks.

**What we defer (Phase 2+):** Hypothetical query expansion requires an LLM call per search—adds latency and cost. Not MVP.

---

## Paper 3: Code Digital Twin

**Source:** arXiv:2503.07967v3, Fudan University, October 2025  
**Key Concept:** Tacit knowledge preservation

### The Problem

Enterprise software carries **tacit knowledge**—design rationales, historical trade-offs, architectural decisions—that exists only in developers' heads or scattered across PRs, Slack, and meeting notes.

> **"Tacit knowledge, including architectural rationales, design trade-offs, and historical context, is often embedded in developer experience or informal artifacts rather than in code. LLMs cannot reliably access or reconstruct this knowledge."**

### The Five Challenges

1. **Intrinsic system complexity** (coupling, cross-cutting concerns)
2. **Physical vs. conceptual gap** (code shows "what", not "why")
3. **Evolutionary path uniqueness** (historical trade-offs)
4. **Undocumented knowledge loss** (senior engineer leaves)
5. **Socio-technical dependencies** (team conventions)

### The Insight: Bidirectional Knowledge Graph

The "Digital Twin" links:
- **Artifacts** → **Concepts**: `payment.ts` ↔ `Resiliency`
- **Concepts** → **Rationale**: `Resiliency` ↔ "We use RetryPolicy because of Issue #402"

### Relevance to Our MCP

**What we already do:**

| Tacit Knowledge Type | Our Implementation |
|---------------------|-------------------|
| Internal libraries exist | `get_component_usage` with counts |
| Pattern preferences | `get_team_patterns` with frequencies |
| Wrapper relationships | Usage ratio detection (847:3) |
| Testing framework | Detected from code, not package.json |

**What we defer (Phase 3+):** Full concept graphs require NLP over docs/PRs/commits—out of scope for MVP.

---

## Paper 4: CACE (Context-Aware CodeLLM Eviction)

**Source:** arXiv:2506.18796v1, Huawei/Queen's University, 2025  
**Key Concept:** Multi-factor file prioritization

### The Problem

LLM context windows are finite. Standard approaches (LRU, recency-based) evict architecture-critical files just because they haven't been touched recently.

### The Insight: Multi-Factor Scoring

Effective context management weighs multiple factors:

1. **Task Criticality**: Is this file needed for latency-sensitive tasks?
2. **Centrality**: Is this file a hub in the dependency graph (high fan-in)?
3. **Future Demand**: Based on current file, what will be needed next?
4. **Reload Cost**: How expensive is it to re-process this file?

Result: Multi-factor eviction reduces cold-start latency by up to **31%** vs LRU.

### Relevance to Our MCP

**What we already do:** Golden Files selection uses multi-factor scoring:

| CACE Factor | Our Golden Files Equivalent |
|-------------|---------------------------|
| Centrality | Import graph analysis |
| Task criticality | Pattern density score (inject + signals + standalone = 5) |
| Future demand | Canonical examples that demonstrate multiple patterns |

---

## Approaches to Avoid (At Least for MVP)

| Research Proposal | Why We Defer |
|------------------|--------------|
| `verify_grounding` tool (API existence checks) | Requires runtime AST parsing against live codebase |
| Hypothetical query expansion | LLM call per search—adds latency and cost |
| Full concept graph with rationale links | Requires NLP over docs/PRs/commits |
| Pattern drift tracking | Requires historical snapshots (Phase 2) |
| Autonomous multi-step planning | 30-35% success rate—stay human-led |

---

## Key Takeaways

### 1. Grounding Prevents Hallucination

Research validates: force the LLM to cite specific evidence before generating. "97% inject()" is grounded. "Prefer inject()" is not.

### 2. Structured Metadata > Raw Code

Indexing component types and classifications outperforms raw code indexing. We're already doing this.

### 3. Tacit Knowledge is THE Gap

The "senior engineer knowledge" we talk about is what the research calls "tacit knowledge." It's a documented problem, not just a marketing claim.

### 4. Multi-Factor Selection Beats Simple Search

Golden Files (pattern density + centrality) is validated by CACE research. One well-chosen file beats 50 mediocre matches.

### 5. Human-Led Remains Optimal

> **"Agentic AI multi-step planning has ~30-35% success rate. Errors compound."**

We optimize for human+AI collaboration, not autonomous operation.

---

## Sources

### Academic Papers

1. **Grounded AI for Code Review** — arXiv:2510.10290v1, October 2025
2. **DeepCodeSeek** — arXiv:2509.25716v1, ServiceNow Research, ECAI-2025
3. **Code Digital Twin** — arXiv:2503.07967v3, Fudan University, October 2025
4. **CACE** — arXiv:2506.18796v1, Huawei/Queen's University, 2025

### Industry Sources (from previous research cycles)

- Stack Overflow 2024: 63.3% cite "lack of codebase context"
- GitClear 2024: AI-generated code doubled code churn
- Thoughtworks Technology Radar: 14-month tracking of agentic AI failures
- Gergely Orosz: "70% problem" in AI-assisted coding

---